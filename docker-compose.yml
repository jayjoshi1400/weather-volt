x-airflow-common: &airflow-common
  build:
    context: ./docker-setup
    dockerfile: Dockerfile-airflow
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__CORE__HOSTNAME_CALLABLE: "airflow.utils.net.get_host_ip_address"
    AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
    PYTHONPATH: "/opt/airflow:/opt/airflow/dags:/home/airflow/.local/lib/python3.8/site-packages"
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./dbt:/opt/airflow/dbt
    - shared-data-volume:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  # Airflow services
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 20s
      timeout: 15s
      retries: 10
      start_period: 90s  
    restart: always

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    depends_on:
      <<: *airflow-common-depends-on

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    restart: always
    depends_on:
      <<: *airflow-common-depends-on

  airflow-init:
    <<: *airflow-common
    user: "0:0"  # Run as root for initialization
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins
        # CHANGE 3: Create data directory for shared volume
        mkdir -p /opt/airflow/data
        mkdir -p /opt/airflow/dbt/logs
        chmod -R 777 /opt/airflow/dbt/logs
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/data
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/dbt
        chmod -R 777 /opt/airflow/logs
        airflow db init
        airflow users create \
          --username ${AIRFLOW_ADMIN_USER} \
          --password ${AIRFLOW_ADMIN_PASSWORD} \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
    depends_on:
      <<: *airflow-common-depends-on

  # # Kafka services
  # zookeeper:
  #   image: confluentinc/cp-zookeeper:7.5.0
  #   ports:
  #     - "2181:2181"
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #   volumes:
  #     - zookeeper-data:/var/lib/zookeeper/data
  #     - zookeeper-log:/var/lib/zookeeper/log
  #   healthcheck:
  #     test: ["CMD", "nc", "-z", "localhost", "2181"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # kafka:
  #   image: confluentinc/cp-kafka:7.5.0
  #   depends_on:
  #     - zookeeper
  #   ports:
  #     - "9092:9092"
  #     - "29092:29092"
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #   volumes:
  #     - kafka-data:/var/lib/kafka/data
  #   healthcheck:
  #     test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  # schema-registry:
  #   image: confluentinc/cp-schema-registry:7.5.0
  #   depends_on:
  #     - kafka
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
  #     SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # # OpenWeatherMap services
  # openweather-producer:
  #   build:
  #     context: ./openweather
  #     dockerfile: Dockerfile.producer
  #   environment:
  #     - KAFKA_BROKER=kafka:9092
  #     - KAFKA_TOPIC=openweathermap-weather
  #     - OPENWEATHERMAP_API_KEY=${OPENWEATHERMAP_API_KEY}
  #   depends_on:
  #     - kafka
  #   restart: unless-stopped
  #   volumes:
  #     - ./openweather:/app

  # openweather-consumer:
  #   build:
  #     context: ./openweather
  #     dockerfile: Dockerfile.consumer
  #   environment:
  #     - KAFKA_BROKER=kafka:9092
  #     - KAFKA_TOPIC=openweathermap-weather
  #     - KAFKA_GROUP=openweathermap-snowflake-group
  #     - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
  #     - SNOWFLAKE_USER=${SNOWFLAKE_USER}
  #     - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
  #     - SNOWFLAKE_DATABASE=ENERGY_DB
  #     - SNOWFLAKE_SCHEMA=RAW
  #     - SNOWFLAKE_WAREHOUSE=ENERGY_WH
  #     - SNOWFLAKE_ROLE=ACCOUNTADMIN
  #     - SNOWFLAKE_TABLE=OPENWEATHERMAP_CURRENT_JSON
  #   depends_on:
  #     - kafka
  #   restart: unless-stopped
  #   volumes:
  #     - ./openweather:/app
      
  # # Spark services
  # spark-master:
  #   image: bitnami/spark:3.5.0
  #   ports:
  #     - "8090:8080"
  #     - "7077:7077"
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   volumes:
  #     - spark-data:/bitnami
  #     - shared-data-volume:/opt/spark/data

  # spark-worker:
  #   image: bitnami/spark:3.5.0
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   volumes:
  #     - spark-worker-data:/bitnami
  #     - shared-data-volume:/opt/spark/data

  # dbt service
  dbt:
    build:
      context: ./dbt
      dockerfile: Dockerfile
    volumes:
      - ./dbt:/usr/app
    environment:
      - DBT_PROFILES_DIR=/usr/app/
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
    depends_on:
      postgres:
        condition: service_healthy


  # Grafana
  grafana:
    image: grafana/grafana-enterprise:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - '3000:3000'
    volumes:
      - grafana-storage:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_PLUGINS_PREINSTALL=grafana-clock-panel,michelin-snowflake-datasource@@https://github.com/michelin/snowflake-grafana-datasource/releases/latest/download/snowflake-grafana-datasource.zip
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=michelin-snowflake-datasource
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  postgres-db-volume:
  # zookeeper-data:
  # zookeeper-log:
  # kafka-data:
  # spark-data:
  # spark-worker-data:
  grafana-storage:
  shared-data-volume:
